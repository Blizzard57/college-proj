{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1\n",
    "\n",
    "In the tutorial you saw how to compute LDA for a two class problem. In this excercise we will work on a multi-class problem. We will be working with the famous Iris dataset that has been deposited on the UCI machine learning repository\n",
    "(https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset:\n",
    "1. Iris-setosa (n=50)\n",
    "2. Iris-versicolor (n=50)\n",
    "3. Iris-virginica (n=50)\n",
    "\n",
    "The four features of the Iris dataset:\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "<img src=\"iris_petal_sepal.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Once dataset is loaded into a pandas data frame object, the first step is to divide dataset into features and corresponding labels and then divide the resultant dataset into training and test sets. The following code divides data into labels and feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:4].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script assigns the first four columns of the dataset i.e. the feature set to X variable while the values in the fifth column (labels) are assigned to the y variable.\n",
    "\n",
    "The following code divides data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "\n",
    "We will now perform feature scaling as part of data preprocessing too. For this task, we will be using scikit learn `StandardScalar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your code below\n",
    "\n",
    "Write you code below to LDA on the IRIS dataset and compute the overall accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ####\n",
    "import import_ipynb\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import pi\n",
    "\n",
    "from utils import get_accuracy, get_prediction, plot_decision_boundary, generate_gifs\n",
    "from utils import plot_2D_input_datapoints, signum, multi_class_signum, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing X_train and absorbing weight b of the hyperplane\n",
    "X_normalized_train = normalize(X_train[:, :2])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_train), 1))\n",
    "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating covariance of an input matrix\n",
    "def calc_cov_matrix(X_input):\n",
    "  n_samples = np.shape(X_input)[0]\n",
    "  cov_matrix = np.array((1 / (n_samples-1)) * (X_input - X_input.mean(axis=0)).T.dot(X_input - X_input.mean(axis=0)))\n",
    "\n",
    "  return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "\n",
    "  \"\"\"Train method for LDA.\n",
    "\n",
    "  Parameters\n",
    "  -----------\n",
    "  X_train: ndarray (num_examples(rows) vs num_features(columns))\n",
    "   Input dataset which LDA will use to obtain optimal weights during training\n",
    "\n",
    "  y_train: ndarray (num_examples(rows) vs class_labels(columns))\n",
    "  \"\"\"\n",
    "  \n",
    "  # Collecting all class 0 and class 1 into separate variables\n",
    "  class_X0 = X_train[np.argwhere(y_train == 0)[:, 0]]\n",
    "  class_X1 = X_train[np.argwhere(y_train == 1)[:, 0]]\n",
    "\n",
    "  # Getting number of examples in each class\n",
    "  num_class_X0_samples = np.shape(class_X0)[0]\n",
    "  num_class_X1_samples = np.shape(class_X1)[0]\n",
    "\n",
    "  # Computing class mean for each label and calculating the difference between them.\n",
    "  class_X0_mean = class_X0.mean(0)\n",
    "  class_X1_mean = class_X1.mean(0)\n",
    "  class_mean_diff = class_X1_mean - class_X0_mean\n",
    "  class_mean_diff = class_mean_diff.reshape((-1, 1))\n",
    "  SB = np.dot(class_mean_diff, class_mean_diff.T)\n",
    "\n",
    "  # Calculating covariance matrix\n",
    "  cov_mat_class_X0 = calc_cov_matrix(class_X0)\n",
    "  cov_mat_class_X1 = calc_cov_matrix(class_X1)\n",
    "  #SW = num_class_X0_samples * cov_mat_class_X0 + num_class_X0_samples * cov_mat_class_X1\n",
    "  SW = cov_mat_class_X0 + cov_mat_class_X1\n",
    "\n",
    "  print(SB)\n",
    "  print(SW)\n",
    "\n",
    "  eigvals, eigvecs = np.linalg.eig(np.linalg.pinv(SW).dot(SB))\n",
    "\n",
    "  # Getting the eigenvectors with the maximum eigenvalue.\n",
    "  idx = eigvals.argsort()[::-1]\n",
    "  eigvals = eigvals[idx][:1]\n",
    "  weights = np.atleast_1d(eigvecs[:, idx])[:, :1]\n",
    "\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = train(X_normalized_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "num_test_samples = np.shape(y_test)[0]\n",
    "\n",
    "y_test_predicted = np.dot(X_test, trained_weights)\n",
    "y_test_predicted[y_test_predicted >= 0] = 1\n",
    "y_test_predicted[y_test_predicted < 0] = 0\n",
    "\n",
    "y_test_predicted = y_test_predicted.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Getting misclassfied points on test set\n",
    "miscls_test_points = np.unique(np.argwhere(y_test_predicted != Y_test)[:, 0])\n",
    "accuracy = 1-(len(miscls_test_points)/num_test_samples)\n",
    "print(\"Accuracy: \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_test, y_test, trained_weights, dataset_type='test', class_label_01_form='on', model_type='LinearDA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}